{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0067cbee-5d9c-4411-a732-044a95de2b78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "events = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\", header=True, inferSchema=True)\n",
    "events_1 = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c21797-5859-4106-aae3-f9917c38bf62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Delta\n",
    "df = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\",header=True ,inferSchema=True)\n",
    "# Create managed table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"events_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4759261-1a9c-4bea-8244-e82a941b3154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL approach\n",
    "spark.sql(\"DROP Table if exists  events_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e424a4-e47f-4214-8ddc-fc21f34c5b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        CREATE TABLE events_table\n",
    "        USING DELTA \n",
    "        AS SELECT * FROM events_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e4c9a6-9079-4f9e-b6ef-4dc90e16172d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If table exist or not\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7e9622-82b8-4f9b-9743-d69fdad1c784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM events_delta\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f7b01d-289d-4e3b-bc07-7d715f9b6a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test schema enforcement \n",
    "# Create wrong schema DataFrame\n",
    "\n",
    "\n",
    "from pyspark.sql import Row \n",
    "\n",
    "wrong_schema_df = spark.createDataFrame([Row(id=2 , name=\"abc\",value=1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb8c093-d863-4ae0-a22a-627b7b7fa401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append it to the table\n",
    "\n",
    "try:\n",
    "    wrong_schema_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"events_table\")\n",
    "except Exception as e:\n",
    "    print(f\"Enforcement schema: {e}\")\n",
    "\n",
    "# Read the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31451f60-5baf-48b0-8f0d-e7656d86fcb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#That's all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0381529a-cc04-4004-8bc6-3fb3a656c829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Prep & Conversion to Delta\n",
    "# Task: Convert CSV to Delta format. Concept: Writing to Delta format creates a _delta_log folder that tracks every transaction.\n",
    "\n",
    "from pyspark.sql.functions import col, lit, concat_ws\n",
    "from delta.tables import *\n",
    "\n",
    "# 0. PREP: Re-combine our datasets (if not already in memory from Day 3)\n",
    "df_full = events.withColumn(\"month\", lit(\"Oct\")) \\\n",
    "                .unionByName(events_1.withColumn(\"month\", lit(\"Nov\")))\n",
    "\n",
    "# Define our storage path (Volumes is the preferred modern location)\n",
    "delta_path = \"/Volumes/workspace/ecommerce/ecommerce_data/delta/events_bronze\"\n",
    "\n",
    "# 1. WRITE TO DELTA\n",
    "# mode(\"overwrite\") replaces existing data safely (ACID transaction)\n",
    "print(f\"ðŸš€ Writing {df_full.count():,} rows to Delta Lake...\")\n",
    "\n",
    "df_full.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(f\"âœ… Data successfully written to: {delta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2618a183-56dd-4e2c-9968-d1750468d0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating Managed Tables (SQL & PySpark)\n",
    "Task: Create Delta tables accessible via SQL. Concept: A \"Managed Table\" means Databricks manages both the metadata and the file storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68575e30-322a-4349-8752-54f10597de49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# METHOD A: PySpark Approach (SaveAsTable)\n",
    "# This registers the dataframe in the Hive Metastore\n",
    "table_name = \"workspace.ecommerce.events_silver\"\n",
    "df_full.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Managed Table created: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174be581-b1ff-45f7-9228-919e582b3d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- # METHOD B: SQL Approach (Querying the table we just created)\n",
    "-- # Magic command allows us to switch languages comfortably\n",
    "SELECT \n",
    "    brand, \n",
    "    count(*) as volume, \n",
    "    format_number(sum(price), 2) as total_sales\n",
    "FROM workspace.ecommerce.events_silver\n",
    "WHERE event_type = 'purchase'\n",
    "GROUP BY brand\n",
    "ORDER BY total_sales DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ab0357-df0b-4090-93dd-3a5f083afaf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Testing Schema Enforcement\n",
    "Task: Test schema enforcement. Concept: Delta Lake is \"schema-on-write.\" If you try to append data with the wrong columns, it throws an error to protect the table's integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c512b4d1-24f3-4057-9e44-df3ae81264a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Create 'Bad' Data (Schema Mismatch)\n",
    "# Our table expects: event_time, event_type, product_id...\n",
    "# We provide: wrong_col_1, wrong_col_2\n",
    "bad_data = [(\"error_test\", 999)]\n",
    "df_bad = spark.createDataFrame(bad_data, [\"wrong_col_name\", \"wrong_value\"])\n",
    "\n",
    "print(\"ðŸ›¡ï¸ Attempting to write bad schema to Delta table...\")\n",
    "\n",
    "try:\n",
    "    # Attempt to Append\n",
    "    df_bad.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    # We catch the error to demonstrate success\n",
    "    print(\"\\nâœ… SUCCESS: Schema Enforcement blocked the write!\")\n",
    "    print(f\"Error Message Snippet: {str(e)[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32984ed-8b0a-4ca7-9bbc-e847878b9198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Duplicates (The MERGE/Upsert)\n",
    "Task: Handle duplicate inserts. Concept: In traditional data lakes, removing duplicates requires rewriting the whole file. In Delta, we use MERGE to perform an \"Upsert\" (Update if exists, Insert if new).\n",
    "\n",
    "Note: Since this dataset lacks a unique Primary Key, we will simulate one using a composite key (User + Time + Product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9133f91-6ef3-4a52-a24a-543b6609d13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Simulate \"New\" Data that contains some duplicates\n",
    "# We take 5 rows that already exist + 1 brand new row\n",
    "existing_rows = df_full.limit(5)\n",
    "new_row_data = [(\"2019-12-01 12:00:00\", \"purchase\", 12345, 100, \"test_code\", \"test_brand\", 99.99, 88888,\"92d96fde-8bb3-4e00-8c23-a032dfed738c\", \"Dec\")]\n",
    "new_row = spark.createDataFrame(new_row_data, df_full.columns)\n",
    "\n",
    "# Combine to create our \"incoming batch\"\n",
    "incoming_batch = existing_rows.union(new_row)\n",
    "\n",
    "print(f\"Incoming Batch Size: {incoming_batch.count()} (5 Duplicates + 1 New)\")\n",
    "\n",
    "# 2. Initialize Delta Table object\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# 3. PERFORM MERGE (Upsert)\n",
    "# Logic: Match on User + Time + Product. \n",
    "# If matched -> Ignore (don't insert duplicate). If not matched -> Insert.\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    incoming_batch.alias(\"source\"),\n",
    "    \"\"\"\n",
    "    target.user_id = source.user_id AND \n",
    "    target.event_time = source.event_time AND \n",
    "    target.product_id = source.product_id\n",
    "    \"\"\"\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "print(\"âœ… Merge Complete. Duplicates were ignored, new row was added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffa3d57-8a35-4f08-b4ca-36ec2e5d35e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verification & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c298cda-a589-4e84-8ae0-44c8c16489cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "delta_path = \"/Volumes/workspace/ecommerce/ecommerce_data/delta/events_bronze\"\n",
    "# Verify the new row exists and we didn't duplicate the existing 5\n",
    "display(\n",
    "    spark.read.format(\"delta\").load(delta_path)\n",
    "    .filter(col(\"brand\") == \"test_brand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ec405a-2f29-461d-9621-45c9455e3084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ§  Key Learnings & Takeaways\n",
    "Performance: Writing to Delta format optimizes the file layout (Parquet + Transaction Log), making reads faster than raw CSV.\n",
    "Safety: We proved that Schema Enforcement prevents accidental data corruption.\n",
    "Upserts: We used the MERGE command to handle duplicates efficiently. This is standard in \"Slowly Changing Dimension\" (SCD) ETL pipelines.\n",
    "Interoperability: We switched seamlessly between PySpark (saveAsTable) and SQL (SELECT) to interact with the same data."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_04_Databricks_IDC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
