{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0a4802-24e1-4fea-b134-dc43101e47c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_1 = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Oct.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_2 = spark.read.csv(\"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\", header=True, inferSchema=True)\n",
    "df_1, df_2\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit, concat, to_timestamp\n",
    "df_1 = df_1.withColumn(\"event_time\", to_timestamp(\"event_time\")).withColumn(\"month\", lit(\"Oct\"))\n",
    "df_2= df_2.withColumn(\"event_time\", to_timestamp(\"event_time\")).withColumn(\"month\", lit(\"Nov\"))\n",
    "df_full = df_1.union(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676bccbb-46ba-4f9f-bb7b-01380dd1fa78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Dataset schema:\")\n",
    "df_full.printSchema()\n",
    "df_full.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb7822a-9b41-4b97-866d-0de24eb83080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate purchases per user-product for join example\n",
    "from pyspark.sql.functions import sum as sum_, count, first\n",
    "purchases = df_full.filter(\"event_type = 'purchase'\") \\\n",
    "    .groupBy(\"user_id\", \"product_id\") \\\n",
    "    .agg(sum_(\"price\").alias(\"total_spent\"), count(\"*\").alias(\"purchase_count\"))\n",
    "\n",
    "# Complex join: purchases with user sessions (left join to keep all purchases)\n",
    "user_sessions = df_full.groupBy(\"user_id\").agg(first(\"user_session\").alias(\"first_session\"))\n",
    "df_joined = purchases.join(user_sessions, \"user_id\", \"left_outer\") \\\n",
    "    .filter(\"total_spent > 10\")  # Complex condition post-join\n",
    "\n",
    "df_joined.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ee8702-2aa8-4710-902b-bee4c85f114c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter purchases and sort\n",
    "purchases_sorted = df_full.filter(\"event_type = 'purchase'\") \\\n",
    "    .select(\"user_id\", \"event_time\", \"price\") \\\n",
    "    .orderBy(\"user_id\", \"event_time\")\n",
    "\n",
    "# Window spec: partition by user, running cumulative\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"event_time\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_running = purchases_sorted.withColumn(\"running_total\", sum_(\"price\").over(window_spec))\n",
    "df_running.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0348a6-5882-466e-89ac-1a3f4fa8114e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# UDF for price tier\n",
    "def price_tier(price):\n",
    "    if price > 100: return \"Premium\"\n",
    "    elif price > 50: return \"Mid\"\n",
    "    else: return \"Budget\"\n",
    "tier_udf = udf(price_tier, StringType())\n",
    "\n",
    "# Extract category depth (e.g., len(split(category_code, '.')))\n",
    "def cat_depth(code):\n",
    "    return len(code.split('.')) if code else 0\n",
    "depth_udf = udf(cat_depth, IntegerType())\n",
    "\n",
    "df_features = df_full.withColumn(\"price_tier\", tier_udf(\"price\")) \\\n",
    "    .withColumn(\"category_depth\", depth_udf(\"category_code\")) \\\n",
    "    .withColumn(\"is_purchase\", (col(\"event_type\") == \"purchase\").cast(\"int\"))\n",
    "\n",
    "df_features.filter(\"event_type = 'purchase'\").select(\"user_id\", \"price_tier\", \"category_depth\", \"is_purchase\").show(10)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_03_Databricks_IDC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
